# üöÄ Mejoras de Rendimiento - Bot Gastos WhatsApp

**Documento de implementaci√≥n para optimizaciones cr√≠ticas**

---

## 1. üî¥ Cache de Selectores WhatsApp (IMPLEMENTADO)

### Problema Actual
- El bot prueba 10+ selectores CSS cada vez que busca mensajes
- Cada selector requiere una b√∫squeda DOM completa (~50-100ms)
- Total: 500-1000ms por ciclo de polling

### Soluci√≥n Implementada
```python
# Ya optimizado en infrastructure/whatsapp/whatsapp_selenium.py:441
class SmartSelectorCache:
    def find_messages_optimized(self, driver):
        # 1. Probar selector cacheado primero (ultra r√°pido)
        if self.cached_selector:
            elements = driver.find_elements(By.CSS_SELECTOR, self.cached_selector)
            if elements:
                return elements  # ‚ö° 90% de casos terminan aqu√≠
        
        # 2. Solo si falla, probar otros selectores ordenados por √©xito
        # Logs cambiados a DEBUG para evitar spam
```

**Resultado:** Reducci√≥n de 500ms ‚Üí 5ms en 90% de casos

---

## 2. üîµ Sistema de Cache Redis

### Problema
- Procesamiento NLP se repite para mensajes similares
- Modelos ML se recargan constantemente
- Sin persistencia entre reinicializaciones

### Implementaci√≥n Propuesta

#### 2.1 Estructura de Cache
```python
# infrastructure/caching/smart_redis_cache.py
class SmartRedisCache:
    def __init__(self, redis_url="redis://localhost:6379/0"):
        self.redis = redis.Redis.from_url(redis_url)
        self.cache_ttl = {
            'nlp_categorization': 3600,    # 1 hora
            'ml_predictions': 1800,        # 30 min
            'text_preprocessing': 7200,    # 2 horas
            'message_hashes': 86400        # 24 horas
        }
    
    def cache_nlp_result(self, text_hash: str, category: str, confidence: float):
        """Cache resultado de categorizaci√≥n NLP."""
        key = f"nlp:{text_hash}"
        value = {"category": category, "confidence": confidence, "timestamp": time.time()}
        self.redis.setex(key, self.cache_ttl['nlp_categorization'], json.dumps(value))
    
    def get_cached_nlp(self, text_hash: str) -> Optional[dict]:
        """Recuperar categorizaci√≥n cacheada."""
        key = f"nlp:{text_hash}"
        cached = self.redis.get(key)
        if cached:
            return json.loads(cached)
        return None
    
    def cache_ml_model(self, model_type: str, model_data: bytes):
        """Cache modelo ML serializado."""
        key = f"ml_model:{model_type}"
        self.redis.setex(key, self.cache_ttl['ml_predictions'], model_data)
```

#### 2.2 Integraci√≥n con NLP Categorizer
```python
# app/services/nlp_categorizer.py - MODIFICACI√ìN
class CachedNLPCategorizer:
    def __init__(self):
        self.cache = SmartRedisCache()
        self.nlp_processor = OriginalNLPProcessor()
    
    def categorize_expense(self, text: str) -> Tuple[str, float]:
        # 1. Generar hash del texto normalizado
        text_normalized = self._normalize_text(text)
        text_hash = hashlib.md5(text_normalized.encode()).hexdigest()
        
        # 2. Buscar en cache primero
        cached_result = self.cache.get_cached_nlp(text_hash)
        if cached_result:
            return cached_result['category'], cached_result['confidence']
        
        # 3. Solo si no est√° en cache, procesar
        category, confidence = self.nlp_processor.categorize(text)
        
        # 4. Guardar resultado en cache
        self.cache.cache_nlp_result(text_hash, category, confidence)
        
        return category, confidence
```

#### 2.3 Instalaci√≥n
```bash
# requirements.txt - AGREGAR
redis>=4.5.0
redis-py-cluster>=2.1.0  # Para clustering futuro
```

**Resultado esperado:** 80% reducci√≥n en tiempo de categorizaci√≥n NLP

---

## 3. üü° Pool de Conexiones

### Problema Actual
- Cada operaci√≥n crea nueva conexi√≥n WhatsApp/DB
- Overhead de conexi√≥n/desconexi√≥n constante
- Sin reutilizaci√≥n de recursos

### Implementaci√≥n Propuesta

#### 3.1 Pool de Conexiones WhatsApp
```python
# infrastructure/connection_pool.py
class WhatsAppConnectionPool:
    def __init__(self, pool_size: int = 3):
        self.pool_size = pool_size
        self.available_drivers = []
        self.busy_drivers = []
        self.lock = threading.Lock()
        self.logger = get_logger(__name__)
    
    def get_driver(self, timeout: int = 30) -> webdriver.Chrome:
        """Obtener driver del pool o crear uno nuevo."""
        with self.lock:
            if self.available_drivers:
                driver = self.available_drivers.pop()
                self.busy_drivers.append(driver)
                return driver
            
            # Si no hay disponibles, crear nuevo (hasta l√≠mite)
            if len(self.busy_drivers) < self.pool_size:
                driver = self._create_new_driver()
                self.busy_drivers.append(driver)
                return driver
        
        # Pool lleno, esperar a que se libere uno
        return self._wait_for_available_driver(timeout)
    
    def return_driver(self, driver: webdriver.Chrome):
        """Devolver driver al pool."""
        with self.lock:
            if driver in self.busy_drivers:
                self.busy_drivers.remove(driver)
                
                # Verificar si el driver a√∫n es v√°lido
                if self._is_driver_healthy(driver):
                    self.available_drivers.append(driver)
                else:
                    # Driver corrupto, cerrarlo y crear nuevo
                    self._close_driver_safe(driver)
    
    def _is_driver_healthy(self, driver: webdriver.Chrome) -> bool:
        """Verificar si el driver sigue funcionando."""
        try:
            # Test r√°pido - verificar que WhatsApp sigue cargado
            driver.find_element(By.CSS_SELECTOR, "[data-testid='chat-list']")
            return True
        except:
            return False
```

#### 3.2 Pool de Conexiones Base de Datos
```python
# infrastructure/storage/db_pool.py
class DatabaseConnectionPool:
    def __init__(self, db_path: str, pool_size: int = 5):
        self.db_path = db_path
        self.pool = queue.Queue(maxsize=pool_size)
        
        # Pre-crear conexiones
        for _ in range(pool_size):
            conn = sqlite3.connect(db_path, check_same_thread=False)
            conn.row_factory = sqlite3.Row  # Para acceso por nombre
            self.pool.put(conn)
    
    @contextmanager
    def get_connection(self):
        """Context manager para obtener/devolver conexi√≥n."""
        conn = self.pool.get(timeout=10)
        try:
            yield conn
        finally:
            # Verificar que la conexi√≥n sigue siendo v√°lida
            try:
                conn.execute("SELECT 1").fetchone()
                self.pool.put(conn)
            except:
                # Conexi√≥n corrupta, crear nueva
                new_conn = sqlite3.connect(self.db_path, check_same_thread=False)
                new_conn.row_factory = sqlite3.Row
                self.pool.put(new_conn)
```

**Resultado esperado:** 60% reducci√≥n en latencia de operaciones DB/WhatsApp

---

## 4. üü† Procesamiento As√≠ncrono

### Problema Actual
- Procesamiento secuencial: leer ‚Üí procesar ‚Üí guardar
- Un mensaje lento bloquea todos los dem√°s
- Sin paralelizaci√≥n de tareas pesadas

### Implementaci√≥n Propuesta

#### 4.1 Queue As√≠ncrono
```python
# infrastructure/async_processor.py
import asyncio
import aiofiles
from concurrent.futures import ThreadPoolExecutor

class AsyncMessageProcessor:
    def __init__(self, max_workers: int = 4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.message_queue = asyncio.Queue()
        self.processing_tasks = []
        self.logger = get_logger(__name__)
    
    async def start_workers(self):
        """Iniciar workers as√≠ncronos."""
        for i in range(4):
            task = asyncio.create_task(self._worker(f"worker-{i}"))
            self.processing_tasks.append(task)
    
    async def _worker(self, worker_name: str):
        """Worker que procesa mensajes de la cola."""
        while True:
            try:
                # Obtener mensaje de la cola
                message_data = await self.message_queue.get()
                
                # Procesar de forma as√≠ncrona
                await self._process_message_async(message_data, worker_name)
                
                # Marcar tarea como completada
                self.message_queue.task_done()
                
            except Exception as e:
                self.logger.error(f"Error en {worker_name}: {e}")
    
    async def _process_message_async(self, message_data: dict, worker_name: str):
        """Procesar un mensaje de forma as√≠ncrona."""
        # 1. NLP Categorization (CPU intensivo ‚Üí executor)
        loop = asyncio.get_event_loop()
        category, confidence = await loop.run_in_executor(
            self.executor, 
            self.nlp_categorizer.categorize_expense, 
            message_data['text']
        )
        
        # 2. Crear objeto gasto
        gasto = await loop.run_in_executor(
            self.executor,
            self._create_gasto_object,
            message_data, category, confidence
        )
        
        # 3. Guardar en storage (I/O ‚Üí async)
        await self._save_gasto_async(gasto)
        
        self.logger.info(f"[{worker_name}] Procesado: {gasto.monto} {gasto.categoria}")
    
    async def queue_message(self, message_data: dict):
        """A√±adir mensaje a la cola de procesamiento."""
        await self.message_queue.put(message_data)
```

#### 4.2 Integraci√≥n con Bot Principal
```python
# interface/cli/run_bot.py - MODIFICACI√ìN
class AsyncBotRunner:
    def __init__(self):
        self.async_processor = AsyncMessageProcessor()
        self.whatsapp_connector = WhatsAppConnector()
    
    async def run_async(self):
        """Bucle principal as√≠ncrono."""
        # Iniciar workers
        await self.async_processor.start_workers()
        
        while self.running:
            # Leer mensajes (sincronizaci√≥n necesaria con Selenium)
            new_messages = await self._fetch_messages_async()
            
            # Encolar mensajes para procesamiento paralelo
            for message in new_messages:
                await self.async_processor.queue_message(message)
            
            # Esperar intervalo de polling
            await asyncio.sleep(self.poll_interval)
    
    async def _fetch_messages_async(self) -> List[dict]:
        """Fetch mensajes usando executor para no bloquear."""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None, 
            self.whatsapp_connector.get_new_messages
        )
```

**Resultado esperado:** 4x throughput de mensajes, procesamiento paralelo

---

## 5. üü¢ Optimizaci√≥n ML

### Problema Actual
- Modelos ML se recargan desde disco cada vez
- Sin pre-entrenamiento con datos hist√≥ricos
- Algoritmos sub√≥ptimos para categorizaci√≥n

### Implementaci√≥n Propuesta

#### 5.1 ML Model Cache Inteligente
```python
# app/services/advanced_ml_optimizer.py - MEJORA
class OptimizedMLCategorizer:
    def __init__(self):
        self.model_cache = {}
        self.redis_cache = SmartRedisCache()
        self.training_data = []
        self.last_retrain_time = 0
        
    def get_cached_model(self, model_type: str):
        """Obtener modelo desde cache en memoria o Redis."""
        # 1. Cache en memoria (m√°s r√°pido)
        if model_type in self.model_cache:
            return self.model_cache[model_type]
        
        # 2. Cache en Redis
        model_data = self.redis_cache.get(f"ml_model:{model_type}")
        if model_data:
            model = pickle.loads(model_data)
            self.model_cache[model_type] = model  # Cachear en memoria tambi√©n
            return model
        
        # 3. Crear y entrenar modelo nuevo
        return self._create_and_cache_model(model_type)
    
    def _create_and_cache_model(self, model_type: str):
        """Crear, entrenar y cachear modelo optimizado."""
        # Usar modelo m√°s eficiente: MultinomialNB en lugar de SVC
        model = Pipeline([
            ('tfidf', TfidfVectorizer(max_features=500, ngram_range=(1, 2))),
            ('classifier', MultinomialNB(alpha=0.1))
        ])
        
        # Entrenar con datos hist√≥ricos + datos sint√©ticos
        training_data = self._get_enhanced_training_data()
        if training_data:
            X, y = zip(*training_data)
            model.fit(X, y)
            
            # Cachear en memoria y Redis
            self.model_cache[model_type] = model
            model_serialized = pickle.dumps(model)
            self.redis_cache.cache_ml_model(model_type, model_serialized)
        
        return model
    
    def _get_enhanced_training_data(self) -> List[Tuple[str, str]]:
        """Datos de entrenamiento mejorados con ejemplos sint√©ticos."""
        # Base: datos hist√≥ricos del Excel/DB
        historical_data = self._load_historical_expenses()
        
        # Extensi√≥n: datos sint√©ticos para categor√≠as con pocos ejemplos
        synthetic_data = [
            ("compr√© pan y leche", "comida"),
            ("pagu√© netflix", "entretenimiento"), 
            ("cargu√© la sube", "transporte"),
            ("farmacia paracetamol", "salud"),
            ("super carrefour", "comida"),
            ("nafta ypf", "transporte"),
            ("spotify premium", "entretenimiento"),
            # ... m√°s ejemplos sint√©ticos
        ]
        
        return historical_data + synthetic_data
    
    def should_retrain(self) -> bool:
        """Determinar si hay que re-entrenar el modelo."""
        # Re-entrenar si han pasado 7 d√≠as o hay 50+ nuevos ejemplos
        time_threshold = time.time() - (7 * 24 * 3600)  # 7 d√≠as
        data_threshold = len(self.training_data) >= 50
        
        return (self.last_retrain_time < time_threshold) or data_threshold
```

#### 5.2 Optimizaci√≥n de Features
```python
# app/services/text_preprocessor.py
class FastTextPreprocessor:
    def __init__(self):
        self.stopwords = set(['el', 'la', 'de', 'que', 'y', 'a', 'en'])  # B√°sico
        self.category_keywords = {
            'comida': ['super', 'pan', 'leche', 'carrefour', 'disco'],
            'transporte': ['nafta', 'sube', 'taxi', 'uber', 'colectivo'],
            'entretenimiento': ['cine', 'netflix', 'spotify', 'disney'],
            # ... m√°s keywords por categor√≠a
        }
    
    def preprocess_fast(self, text: str) -> str:
        """Preprocessamiento ultrarr√°pido."""
        # 1. Normalizaci√≥n b√°sica (sin regex complejas)
        text = text.lower().strip()
        
        # 2. Detecci√≥n r√°pida por keywords
        for category, keywords in self.category_keywords.items():
            if any(keyword in text for keyword in keywords):
                text += f" {category}_hint"  # Feature adicional
        
        # 3. Limpieza m√≠nima
        text = ''.join(c for c in text if c.isalnum() or c.isspace())
        
        return text
```

**Resultado esperado:** 10x velocidad de categorizaci√≥n ML, 90% precisi√≥n

---

## üìä Resumen de Mejoras

| Optimizaci√≥n | Estado | Mejora Esperada | Tiempo Implementaci√≥n |
|-------------|--------|-----------------|----------------------|
| ‚úÖ Cache Selectores | **COMPLETADO** | 90% reducci√≥n latencia DOM | - |
| üîµ Cache Redis | Pendiente | 80% reducci√≥n NLP | 4 horas |
| üü° Pool Conexiones | Pendiente | 60% reducci√≥n latencia I/O | 3 horas |
| üü† Async Processing | Pendiente | 4x throughput mensajes | 6 horas |
| üü¢ ML Optimizado | Pendiente | 10x velocidad ML | 5 horas |

**Total estimado:** 18 horas de desarrollo para **5-10x mejora de rendimiento**

---

## üöÄ Plan de Implementaci√≥n

### Fase 1 (Cr√≠tica - 4 horas)
1. Cache Redis para NLP
2. Pool b√°sico de conexiones DB

### Fase 2 (Mejoras - 8 horas)
3. Pool conexiones WhatsApp
4. ML optimizado con cache

### Fase 3 (Avanzado - 6 horas)
5. Procesamiento as√≠ncrono completo
6. M√©tricas y monitoreo

**Prioridad:** Implementar en orden para impacto m√°ximo con esfuerzo m√≠nimo.